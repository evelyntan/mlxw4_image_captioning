{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPProcessor, CLIPTokenizer, CLIPModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize CLIP processor and tokenizer\n",
    "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n",
    "model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Current device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, dataset, clip_model_name=\"openai/clip-vit-base-patch32\", device=device):\n",
    "        self.image = dataset['image']\n",
    "        self.caption_list = dataset['caption']\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)\n",
    "        self.clip_model = CLIPModel.from_pretrained(clip_model_name).eval().to(self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image[idx]\n",
    "        caption_list = self.caption_list[idx]\n",
    "        \n",
    "        # ---- Encode image with CLIP ----\n",
    "        img_tensor = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "        #print('IMG TENSOR SHAPE', img_tensor.shape) # channels, height, width\n",
    "        \n",
    "        # ---- Tokenize input caption ----\n",
    "        caption = caption_list[0] # get the first caption in the list\n",
    "        #print('caption len:', len(caption))\n",
    "        tokens = self.tokenizer(caption, padding=\"max_length\", max_length=32, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "        input_ids_full = tokens[\"input_ids\"].to(self.device)  # [1, seq_len]\n",
    "        #print('text_input_ids_full shape:', input_ids_full.shape)\n",
    "        mask = tokens[\"attention_mask\"].to(self.device) # get the mask out\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Use only embedding layer from CLIP\n",
    "            text_embeddings = self.clip_model.text_model.embeddings(input_ids_full).squeeze(0).to(self.device)\n",
    "\n",
    "            # Get the CLIP encoded image embeddings\n",
    "            patch_embeddings = self.clip_model.vision_model(**img_tensor).last_hidden_state[:, 1:, :].squeeze(0).to(self.device) # shape: [1, num_patches, hidden_dim]\n",
    "            #print('Patch embeddings shape:', patch_embeddings.shape)           \n",
    "            \n",
    "            \n",
    "        target_ids = input_ids_full.squeeze(0).to(self.device)\n",
    "\n",
    "        #print('IMG EMBEDDINGS SHAPE', patch_embeddings.shape)\n",
    "        #print('TEXT EMBEDDINGS SHAPE', text_embeddings.shape)\n",
    "        #print('TARGET IDS SHAPE', target_ids.shape)\n",
    "        #print('MASK SHAPE', mask.shape)\n",
    "\n",
    "        return patch_embeddings, text_embeddings, target_ids, mask\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Iterate over the DataLoader and print the outputs\n",
    "for batch_idx, (patch_embeddings, text_embeddings, target_ids, mask) in enumerate(test_dataloader):\n",
    "    print('\\nImage Embeddings shape:', patch_embeddings.shape)\n",
    "    print('Text Embeddings shape:', text_embeddings.shape)\n",
    "\n",
    "    # Move all to device\n",
    "    patch_embeddings = patch_embeddings.to(device)\n",
    "    text_embeddings = text_embeddings.to(device)\n",
    "    target_ids = target_ids.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    image_input_dim = patch_embeddings.shape[2]# get dimension from patch embeddings\n",
    "    text_embed_dim = text_embeddings.shape[2]\n",
    "\n",
    "    # Move projection layer to device\n",
    "    image_projection_layer = nn.Linear(image_input_dim, text_embed_dim).to(device)\n",
    "    img_features = image_projection_layer(patch_embeddings)\n",
    "    print('\\nProjected img shape:', img_features.shape)\n",
    "    print('Projected text shape:', text_embeddings.shape)\n",
    "\n",
    "    print('Text tensor type:', text_embeddings.dtype)\n",
    "    print('Image tensor type:', img_features.dtype)\n",
    "    break  # Remove this break to iterate over the entire subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create the image captioning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬆️ The code above projects the text and image embeddings into the same shape so now we can send it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Masked Self Attention Head\n",
    "class MaskedAttentionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_dim):\n",
    "        super(MaskedAttentionHead, self).__init__()\n",
    "        self.head_dim = head_dim\n",
    "\n",
    "        # Linear projections for query, key, value\n",
    "        self.weight_q = nn.Linear(embedding_dim, head_dim)\n",
    "        self.weight_k = nn.Linear(embedding_dim, head_dim)\n",
    "        self.weight_v = nn.Linear(embedding_dim, head_dim)\n",
    "\n",
    "        self.linear_projection = nn.Linear(head_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, decoder_sequence):\n",
    "        # embedded decoder sequence shape: [batch_size, seq_length, embedding_dim]\n",
    "\n",
    "        # Project to head dimension\n",
    "        Q = self.weight_q(decoder_sequence)\n",
    "        K = self.weight_k(decoder_sequence)\n",
    "        V = self.weight_v(decoder_sequence)\n",
    "\n",
    "        # Make the mask\n",
    "        seq_len = decoder_sequence.shape[1]\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=decoder_sequence.device), diagonal=1)\n",
    "        mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "\n",
    "        # Calculate attention scores (scaled dot product)\n",
    "        A = torch.einsum('bid,bjd->bij', Q, K)\n",
    "        A = A / (self.head_dim ** 0.5) \n",
    "\n",
    "        A = A + mask\n",
    "        # Apply softmax\n",
    "        A = torch.softmax(A, dim=-1)\n",
    "\n",
    "    \n",
    "        #  Apply attention weights to values\n",
    "        H = torch.einsum('bij,bjd->bid', A, V)\n",
    "        \n",
    "        # Add projection layer for output to return back to the original embedding dimension\n",
    "        #output = self.linear_projection(H)\n",
    "\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "\n",
    "     \n",
    "        self.heads = nn.ModuleList(\n",
    "            [MaskedAttentionHead(embedding_dim, self.head_dim) for _ in range(num_heads)]\n",
    "            )\n",
    "        \n",
    "        # The output of the CrossAttention Head and MaskedAttentionHead still needs to be projected\n",
    "        # Back to the embedding dimensions of the head_dim x vocab_size\n",
    "        \n",
    "        self.output_projection = nn.Linear(num_heads * self.head_dim, embedding_dim)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, decoder_sequence):\n",
    "        # decoder_sequence: [batch_size, seq_length, embedding_dim]\n",
    "        # encoder_output: [batch_size, num_patches, embedding_dim] (only used in cross-attention)\n",
    "        # mask: [batch_size, seq_length, seq_length] (only used in self-attention)\n",
    "\n",
    "        # Process each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "                # For masked self-attention, we only need decoder sequence and mask\n",
    "                head_output = head(decoder_sequence)\n",
    "                #print(\"\\nmasked attention head output shape: \", head_output.shape)\n",
    "                \n",
    "                head_outputs.append(head_output)\n",
    "\n",
    "        # Concatenate head outputs\n",
    "        concat_heads = torch.cat(head_outputs, dim=-1)\n",
    "        \n",
    "        # Project back to embedding dimension\n",
    "        output = self.output_projection(concat_heads)\n",
    "        #print(\"Multihead attention output shape: \", output.shape)\n",
    "        \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, mlp_dimension):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        \n",
    "        # First layer norm\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # Masked multi-head attention for decoder sequence self-attention\n",
    "        self.masked_mha = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        \n",
    "        \n",
    "        # Third layer norm\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # Feed forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, mlp_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_dimension, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, decoder_sequence):\n",
    "        # decoder_sequence: the input sequence to decode (e.g., [START, 1, 2, 3])\n",
    "        # encoder_output: the encoded image from the encoder\n",
    "        # mask: causal mask to prevent attending to future tokens\n",
    "\n",
    "        # First masked self-attention block with residual connection\n",
    "        # This allows decoder sequence to attend to its own past tokens\n",
    "        # First masked self-attention\n",
    "        residual = decoder_sequence\n",
    "        decoder_sequence = self.ln1(decoder_sequence)\n",
    "        decoder_sequence = self.masked_mha(decoder_sequence)\n",
    "        decoder_sequence = residual + decoder_sequence\n",
    "\n",
    "        \n",
    "        # # FFN block with residual connection\n",
    "        residual = decoder_sequence\n",
    "        decoder_sequence = self.ln2(decoder_sequence)\n",
    "        decoder_sequence = self.ffn(decoder_sequence)\n",
    "        decoder_sequence = residual + decoder_sequence\n",
    "        \n",
    "        return decoder_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, mlp_dimension, num_layers, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "        # Create positional embeddings for decoder sequence ONCE during initialization\n",
    "        #self.positional_embeddings = nn.Parameter(\n",
    "        #    torch.randn(1, input_sequence_length, embedding_dim),\n",
    "        #    requires_grad=True\n",
    "        #)\n",
    "        \n",
    "        # Create decoder blocks\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderBlock(embedding_dim, num_heads, mlp_dimension)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.final_ln = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Output projection to vocabulary size\n",
    "        # This converts decoder features to logits over possible next tokens\n",
    "        self.output_projection = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, concatenated_embeddings, return_logits=True):\n",
    "        # return_logits: whether to return prediction logits or just decoder features\n",
    "        # by default, we return logits\n",
    "\n",
    "        # Add positional embeddings to decoder sequence\n",
    "        #decoder_sequence = embedded_decoder_sequence + self.positional_embeddings\n",
    "        \n",
    "        # Pass through decoder blocks\n",
    "        for block in self.decoder_blocks:\n",
    "            concatenated_embeddings = block(concatenated_embeddings)\n",
    "        \n",
    "        # Apply final layer norm\n",
    "        decoder_features = self.final_ln(concatenated_embeddings)\n",
    "        \n",
    "        if return_logits:\n",
    "            # Convert features to logits for prediction\n",
    "            # Shape: [batch_size, seq_length, vocab_size]\n",
    "            logits = self.output_projection(decoder_features)\n",
    "            return logits\n",
    "        else:\n",
    "            # Return decoder features if needed\n",
    "            return decoder_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
      "    num_rows: 1500\n",
      "})\n",
      "Dataset({\n",
      "    features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
      "    num_rows: 3500\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# load dataset and create train and test sets\n",
    "raw_dataset = load_dataset(\"nlphuji/flickr30k\", split='test[:5000]')\n",
      "\n",
    "# Split the dataset into training and testing sets\n",
    "train_test_split = raw_dataset.train_test_split(test_size=0.3)\n",
    "train = train_test_split['train']\n",
    "test = train_test_split['test']\n",
      "\n",
    "print(test)\n",
    "print(train)\n",
      "\n",
    "test_caption = CaptionDataset(test)\n",
    "train_caption = CaptionDataset(train)\n",
      "\n",
    "# Create a DataLoader\n",
    "test_dataloader = DataLoader(test_caption, batch_size=1, shuffle=False)  # Adjust batch_size as needed\n",
    "train_dataloader = DataLoader(train_caption, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]:  91%|█████████ | 3188/3500 [02:12<00:12, 24.10it/s, batch_loss=1.4329]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m epoch_losses \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# store all individual losses and calculate the true average at the end\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Iterate over the DataLoader and print the outputs\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (patch_embeddings, text_embeddings, target_ids, mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_pbar):\n\u001b[1;32m     32\u001b[0m     \n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Move projection layer to device\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     image_input_dim \u001b[38;5;241m=\u001b[39m patch_embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;66;03m# get dimension from patch embeddings\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     image_projection_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(image_input_dim, embedding_dim)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m, in \u001b[0;36mCaptionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m     text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_model\u001b[38;5;241m.\u001b[39mtext_model\u001b[38;5;241m.\u001b[39membeddings(input_ids_full)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Get the CLIP encoded image embeddings\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     patch_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m1\u001b[39m:, :]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;66;03m# shape: [1, num_patches, hidden_dim]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#print('Patch embeddings shape:', patch_embeddings.shape)           \u001b[39;00m\n\u001b[1;32m     41\u001b[0m target_ids \u001b[38;5;241m=\u001b[39m input_ids_full\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:1094\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m   1091\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding)\n\u001b[1;32m   1092\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[0;32m-> 1094\u001b[0m encoder_outputs: BaseModelOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m   1101\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:882\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    874\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    875\u001b[0m         encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    876\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    879\u001b[0m         output_attentions,\n\u001b[1;32m    880\u001b[0m     )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 882\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:623\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    622\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 623\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm2\u001b[49m(hidden_states)\n\u001b[1;32m    624\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    625\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/mlxw4_image_captioning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1927\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1922\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[0;32m-> 1927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1928\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1929\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parameters for decoder\n",
    "embedding_dim = 512\n",
    "num_heads = 8\n",
    "mlp_dimension = 2048\n",
    "num_layers = 2\n",
    "input_sequence_length = 32\n",
    "vocab_size = 49408\n",
    "\n",
    "decoder = Decoder(embedding_dim, num_heads, mlp_dimension, num_layers, vocab_size).to(device)\n",
    "#decoder = SimpleCaptionDecoder(embedding_dim, num_heads, mlp_dimension, num_layers, vocab_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    decoder.train()\n",
    "\n",
    "    # Training phase\n",
    "    train_pbar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', mininterval=5.0)\n",
    "\n",
    "    epoch_losses = [] # store all individual losses and calculate the true average at the end\n",
    "\n",
    "    # Iterate over the DataLoader and print the outputs\n",
    "    for batch_idx, (patch_embeddings, text_embeddings, target_ids, mask) in enumerate(train_pbar):\n",
    "        \n",
    "        # Move projection layer to device\n",
    "        image_input_dim = patch_embeddings.shape[2]# get dimension from patch embeddings\n",
    "        image_projection_layer = nn.Linear(image_input_dim, embedding_dim).to(device)\n",
    "        img_features = image_projection_layer(patch_embeddings)\n",
    "        #print('\\nProjected img shape:', img_features.shape)\n",
    "        #print('Projected text shape:', text_embeddings.shape)\n",
    "\n",
    "        # Concatenate image features and text embeddings\n",
    "        decoder_inputs = torch.cat([img_features, text_embeddings], dim=1).to(device)\n",
    "        targets = target_ids.to(device)\n",
    "\n",
    "        # Forward pass through the decoder\n",
    "        logits = decoder(decoder_inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        # Shift logits and targets to align for cross-entropy\n",
    "        #logits = logits[:, :-1].contiguous().view(-1, logits.size(-1))\n",
    "        #targets = target_ids[:, 1:].contiguous().view(-1)\n",
    "        #loss = criterion(logits, targets)\n",
    "\n",
    "        # Calculate loss only on text portion\n",
    "        text_logits = logits[:, -32:, :] # Shape: [batch_size, 32, vocab_size]\n",
    "        text_targets = target_ids[:, -32:]  # Shape: [batch_size, 32]\n",
    "        text_mask = mask[:, -32:] # Shape: [batch_size, 32]\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        text_logits = text_logits.reshape(-1, text_logits.size(-1))\n",
    "        text_targets = text_targets.reshape(-1)\n",
    "        text_mask = text_mask.reshape(-1)\n",
    "\n",
    "        # Calculate loss only on masked positions\n",
    "        loss = criterion(\n",
    "            text_logits[text_mask.bool()],\n",
    "            text_targets[text_mask.bool()]\n",
    "        )\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Store the loss for this batch\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        # Update progress bar with current batch loss\n",
    "        train_pbar.set_postfix({\n",
    "            'batch_loss': f'{loss.item():.4f}'\n",
    "        })\n",
    "        train_pbar.update(1)\n",
    "\n",
    "    # Close progress bar for this epoch\n",
    "    train_pbar.close()\n",
    "    \n",
    "    # Print epoch summary\n",
    "    # Calculate and print training epoch average loss\n",
    "    avg_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "     # Validation phase\n",
    "    decoder.eval()  # Set model to evaluation mode\n",
    "    val_losses = []  # Store all validation losses\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        val_pbar = tqdm(test_dataloader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]', mininterval=5.0)\n",
    "        \n",
    "        for batch_idx, (patch_embeddings, text_embeddings, target_ids, mask) in enumerate(val_pbar):\n",
    "            # Move projection layer to device\n",
    "            image_input_dim = patch_embeddings.shape[2]\n",
    "            image_projection_layer = nn.Linear(image_input_dim, embedding_dim).to(device)\n",
    "            img_features = image_projection_layer(patch_embeddings)\n",
    "\n",
    "            # Concatenate image features and text embeddings\n",
    "            decoder_inputs = torch.cat([img_features, text_embeddings], dim=1).to(device)\n",
    "            targets = target_ids.to(device)\n",
    "\n",
    "            # Forward pass through the decoder\n",
    "            logits = decoder(decoder_inputs)\n",
    "\n",
    "            # Calculate loss only on text portion\n",
    "            text_logits = logits[:, -32:, :]\n",
    "            text_targets = target_ids[:, -32:]\n",
    "            text_mask = mask[:, -32:]\n",
    "\n",
    "            # Reshape for loss calculation\n",
    "            text_logits = text_logits.reshape(-1, text_logits.size(-1))\n",
    "            text_targets = text_targets.reshape(-1)\n",
    "            text_mask = text_mask.reshape(-1)\n",
    "\n",
    "            # Calculate loss only on masked positions\n",
    "            val_loss = criterion(\n",
    "                text_logits[text_mask.bool()],\n",
    "                text_targets[text_mask.bool()]\n",
    "            )\n",
    "            \n",
    "            # Store validation loss\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Update progress bar with current validation loss\n",
    "            val_pbar.set_postfix({\n",
    "                'val_loss': f'{val_loss.item():.4f}'\n",
    "            })\n",
    "            val_pbar.update(1)\n",
    "    \n",
    "    # Close validation progress bar\n",
    "    val_pbar.close()\n",
    "    \n",
    "    # Calculate average validation loss\n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "print(\"Training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DUMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "\n",
    "def train_transformer(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "):\n",
    "    \n",
    "    # Initialize wandb\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_name = f\"full-transformer-{timestamp}\"\n",
    "    wandb.init(project=\"full-transformer-mnist\",\n",
    "               name=run_name,\n",
    "               group=\"experiment-1\",\n",
    "               config={\n",
    "                   \"learning_rate\": learning_rate,\n",
    "                   \"num_epochs\": num_epochs,\n",
    "                   \"batch_size\": train_loader.batch_size,\n",
    "                   \"embedding_dim\": model.embedding_dim,\n",
    "                   \"num_heads\": model.num_heads,\n",
    "                   \"mlp_dimension\": model.mlp_dimension,\n",
    "                   \"num_layers\": model.num_layers,\n",
    "                   \"max_seq_length\": model.num_patches,\n",
    "                   \"vocab_size\": model.vocab_size\n",
    "               })\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # TRAINING LOOP \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Training phase\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', mininterval=1.0) # udpate every 1 second\n",
    "        for batch_idx, (input_images, decoder_inputs, targets) in enumerate(train_loader):\n",
    "            # Move to device\n",
    "            input_images = input_images.to(device)\n",
    "            decoder_inputs = decoder_inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_images, decoder_inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            # outputs.view(-1, outputs.size(-1)) is to flatten the output to a 1D tensor\n",
    "            # targets.view(-1) is to flatten the targets to a 1D tensor\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(-1)\n",
    "            total_correct += (predicted == targets).sum().item()\n",
    "            total_samples += targets.numel()\n",
    "            \n",
    "            # Calculate current accuracy\n",
    "            current_acc = (total_correct / total_samples * 100)\n",
    "            \n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({\n",
    "                'loss': total_loss / (batch_idx + 1),\n",
    "                'acc': f'{current_acc:.2f}%'\n",
    "            })\n",
    "\n",
    "            train_pbar.update(1)\n",
    "        \n",
    "        train_pbar.close()\n",
    "\n",
    "        \n",
    "        # Calculate training metrics after all batches\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = (total_correct / total_samples * 100)\n",
    "        print(f\"\\nTraining completed for epoch {epoch+1}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "        # After training phase\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "            \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "        \n",
    "        # No need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]', mininterval=1.0)\n",
    "            for batch_idx, (images, decoder_input, targets) in enumerate(val_pbar):\n",
    "                # Move to device\n",
    "                images = images.to(device)\n",
    "                decoder_input = decoder_input.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images, decoder_input)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Update validation metrics\n",
    "                _, predicted = outputs.max(-1)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "                val_samples += targets.numel()\n",
    "\n",
    "                # Calculate current validation accuracy\n",
    "                current_val_acc = (val_correct / val_samples * 100)\n",
    "                \n",
    "\n",
    "                # Update progress bar\n",
    "                val_pbar.set_postfix({\n",
    "                    'loss': val_loss / (batch_idx + 1),\n",
    "                    'acc': f'{current_val_acc:.2f}%'\n",
    "                })\n",
    "\n",
    "                val_pbar.update(1)\n",
    "\n",
    "        val_pbar.close()\n",
    "                \n",
    "        # Calculate validation metrics\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = (val_correct / val_samples * 100)\n",
    "\n",
    "        # After validation phase\n",
    "        wandb.log({\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "        \n",
    "\n",
    "      # Print epoch summary\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs} Summary:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "    # After training loop completes\n",
    "    # 1. Save model weights\n",
    "    model_save_path = f\"transformer_weights_{timestamp}_mnistscattered.pt\"\n",
    "\n",
    "    torch.save(model, model_save_path)\n",
    "\n",
    "    print(f\"\\nModel weights saved to {model_save_path}\")\n",
    "\n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Assuming 'data' is one sample from your dataset (e.g., a batch or single entry)\n",
    "image = train['image'][0]  # This will be a PIL.Image object\n",
    "\n",
    "# Check the original image size\n",
    "print(\"Original image size:\", image.size)  # Returns (width, height)\n",
    "\n",
    "# Optionally, check the mode (RGB, grayscale, etc.)\n",
    "print(\"Image mode:\", image.mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions produces a list of captions\n",
    "#captions = [caption for caption_list[0] for caption_list in test['caption']]  \n",
    "\n",
    "captions = []\n",
    "for caption_list in test['caption']:\n",
    "    for caption in caption_list[0]:\n",
    "        captions.append(caption)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change from total_loss and num_batches to epoch_losses = [] offers several advantages:\n",
    "More Accurate Average:\n",
    "With total_loss and num_batches, you're calculating a running average that might be affected by the order of batches\n",
    "With epoch_losses, you store all individual losses and calculate the true average at the end, which is more accurate\n",
    "More Information:\n",
    "By storing all losses in a list, you can:\n",
    "Calculate the median loss\n",
    "Find the minimum and maximum losses\n",
    "Plot the loss distribution\n",
    "Detect outliers or unusual patterns\n",
    "Calculate standard deviation\n",
    "Debugging and Analysis:\n",
    "If you notice something unusual in the average loss, you can look at the individual losses to understand what's happening\n",
    "You can track how the loss changes throughout the epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
