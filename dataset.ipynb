{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPProcessor, CLIPTokenizer, CLIPModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
      "    num_rows: 1500\n",
      "})\n",
      "Dataset({\n",
      "    features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
      "    num_rows: 3500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# load dataset and create train and test sets\n",
    "raw_dataset = load_dataset(\"nlphuji/flickr30k\", split='test[:5000]')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_test_split = raw_dataset.train_test_split(test_size=0.3)\n",
    "train = train_test_split['train']\n",
    "test = train_test_split['test']\n",
    "\n",
    "print(test)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize CLIP processor and tokenizer\n",
    "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n",
    "model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Current device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, dataset, clip_model_name=\"openai/clip-vit-base-patch32\", device=device):\n",
    "        self.image = dataset['image']\n",
    "        self.caption_list = dataset['caption']\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)\n",
    "        self.clip_model = CLIPModel.from_pretrained(clip_model_name).eval().to(self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image[idx]\n",
    "        caption_list = self.caption_list[idx]\n",
    "        \n",
    "        # ---- Encode image with CLIP ----\n",
    "        img_tensor = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "        #print('IMG TENSOR SHAPE', img_tensor.shape) # channels, height, width\n",
    "        \n",
    "        # ---- Tokenize input caption ----\n",
    "        caption = caption_list[0] # get the first caption in the list\n",
    "        #print('caption len:', len(caption))\n",
    "        tokens = self.tokenizer(caption, padding=\"max_length\", max_length=32, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "        input_ids_full = tokens[\"input_ids\"].to(self.device)  # [1, seq_len]\n",
    "        #print('text_input_ids_full shape:', input_ids_full.shape)\n",
    "        mask = tokens[\"attention_mask\"].to(self.device) # get the mask out\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Use only embedding layer from CLIP\n",
    "            text_embeddings = self.clip_model.text_model.embeddings(input_ids_full).squeeze(0).to(self.device)\n",
    "\n",
    "            # Get the CLIP encoded image embeddings\n",
    "            patch_embeddings = self.clip_model.vision_model(**img_tensor).last_hidden_state[:, 1:, :].squeeze(0).to(self.device) # shape: [1, num_patches, hidden_dim]\n",
    "            #print('Patch embeddings shape:', patch_embeddings.shape)           \n",
    "            \n",
    "            \n",
    "        target_ids = input_ids_full.squeeze(0).to(self.device)\n",
    "\n",
    "        #print('IMG EMBEDDINGS SHAPE', patch_embeddings.shape)\n",
    "        #print('TEXT EMBEDDINGS SHAPE', text_embeddings.shape)\n",
    "        #print('TARGET IDS SHAPE', target_ids.shape)\n",
    "        #print('MASK SHAPE', mask.shape)\n",
    "\n",
    "        return patch_embeddings, text_embeddings, target_ids, mask\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_caption = CaptionDataset(test)\n",
    "train_caption = CaptionDataset(train)\n",
    "\n",
    "# Create a DataLoader\n",
    "test_dataloader = DataLoader(test_caption, batch_size=10, shuffle=False)  # Adjust batch_size as needed\n",
    "train_dataloader = DataLoader(train_caption, batch_size=10, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text tensor type: torch.float32\n",
      "Image tensor type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the DataLoader and print the outputs\n",
    "for batch_idx, (patch_embeddings, text_embeddings, target_ids, mask) in enumerate(test_dataloader):\n",
    "    print('\\nImage Embeddings shape:', patch_embeddings.shape)\n",
    "    print('Text Embeddings shape:', text_embeddings.shape)\n",
    "\n",
    "    # Move all to device\n",
    "    patch_embeddings = patch_embeddings.to(device)\n",
    "    text_embeddings = text_embeddings.to(device)\n",
    "    target_ids = target_ids.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    image_input_dim = patch_embeddings.shape[2]# get dimension from patch embeddings\n",
    "    text_embed_dim = text_embeddings.shape[2]\n",
    "\n",
    "    # Move projection layer to device\n",
    "    image_projection_layer = nn.Linear(image_input_dim, text_embed_dim).to(device)\n",
    "    img_features = image_projection_layer(patch_embeddings)\n",
    "    print('\\nProjected img shape:', img_features.shape)\n",
    "    print('Projected text shape:', text_embeddings.shape)\n",
    "\n",
    "    print('Text tensor type:', text_embeddings.dtype)\n",
    "    print('Image tensor type:', img_features.dtype)\n",
    "    break  # Remove this break to iterate over the entire subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create the image captioning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'patch_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m image_input_dim = \u001b[43mpatch_embeddings\u001b[49m.squeeze(\u001b[32m1\u001b[39m).shape[\u001b[32m2\u001b[39m] \u001b[38;5;66;03m# get dimension from patch embeddings\u001b[39;00m\n\u001b[32m      4\u001b[39m text_embed_dim = text_embeddings.squeeze(\u001b[32m1\u001b[39m).shape[\u001b[32m2\u001b[39m]\n\u001b[32m      6\u001b[39m image_projection_layer = nn.Linear(image_input_dim, text_embed_dim)\n",
      "\u001b[31mNameError\u001b[39m: name 'patch_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "image_input_dim = patch_embeddings.squeeze(1).shape[2] # get dimension from patch embeddings\n",
    "text_embed_dim = text_embeddings.squeeze(1).shape[2]\n",
    "\n",
    "image_projection_layer = nn.Linear(image_input_dim, text_embed_dim)\n",
    "img_features = image_projection_layer(patch_embeddings)\n",
    "print('\\nProjected img shape:', img_features.shape)\n",
    "print('Projected text shape:', text_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬆️ The code above projects the text and image embeddings into the same shape so now we can send it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Masked Self Attention Head\n",
    "class MaskedAttentionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_dim):\n",
    "        super(MaskedAttentionHead, self).__init__()\n",
    "        self.head_dim = head_dim\n",
    "\n",
    "        # Linear projections for query, key, value\n",
    "        self.weight_q = nn.Linear(embedding_dim, head_dim)\n",
    "        self.weight_k = nn.Linear(embedding_dim, head_dim)\n",
    "        self.weight_v = nn.Linear(embedding_dim, head_dim)\n",
    "\n",
    "        self.linear_projection = nn.Linear(head_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, decoder_sequence):\n",
    "        # embedded decoder sequence shape: [batch_size, seq_length, embedding_dim]\n",
    "\n",
    "        # Project to head dimension\n",
    "        Q = self.weight_q(decoder_sequence)\n",
    "        K = self.weight_k(decoder_sequence)\n",
    "        V = self.weight_v(decoder_sequence)\n",
    "\n",
    "        # Make the mask\n",
    "        seq_len = decoder_sequence.shape[1]\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=decoder_sequence.device), diagonal=1)\n",
    "        mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "\n",
    "        # Calculate attention scores (scaled dot product)\n",
    "        A = torch.einsum('bid,bjd->bij', Q, K)\n",
    "        A = A / (self.head_dim ** 0.5) \n",
    "\n",
    "        A = A + mask\n",
    "        # Apply softmax\n",
    "        A = torch.softmax(A, dim=-1)\n",
    "\n",
    "    \n",
    "        #  Apply attention weights to values\n",
    "        H = torch.einsum('bij,bjd->bid', A, V)\n",
    "        \n",
    "        # Add projection layer for output to return back to the original embedding dimension\n",
    "        #output = self.linear_projection(H)\n",
    "\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "\n",
    "     \n",
    "        self.heads = nn.ModuleList(\n",
    "            [MaskedAttentionHead(embedding_dim, self.head_dim) for _ in range(num_heads)]\n",
    "            )\n",
    "        \n",
    "        # The output of the CrossAttention Head and MaskedAttentionHead still needs to be projected\n",
    "        # Back to the embedding dimensions of the head_dim x vocab_size\n",
    "        \n",
    "        self.output_projection = nn.Linear(num_heads * self.head_dim, embedding_dim)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, decoder_sequence):\n",
    "        # decoder_sequence: [batch_size, seq_length, embedding_dim]\n",
    "        # encoder_output: [batch_size, num_patches, embedding_dim] (only used in cross-attention)\n",
    "        # mask: [batch_size, seq_length, seq_length] (only used in self-attention)\n",
    "\n",
    "        # Process each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "                # For masked self-attention, we only need decoder sequence and mask\n",
    "                head_output = head(decoder_sequence)\n",
    "                #print(\"\\nmasked attention head output shape: \", head_output.shape)\n",
    "                \n",
    "                head_outputs.append(head_output)\n",
    "\n",
    "        # Concatenate head outputs\n",
    "        concat_heads = torch.cat(head_outputs, dim=-1)\n",
    "        \n",
    "        # Project back to embedding dimension\n",
    "        output = self.output_projection(concat_heads)\n",
    "        #print(\"Multihead attention output shape: \", output.shape)\n",
    "        \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, mlp_dimension):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        \n",
    "        # First layer norm\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # Masked multi-head attention for decoder sequence self-attention\n",
    "        self.masked_mha = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        \n",
    "        \n",
    "        # Third layer norm\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # Feed forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, mlp_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_dimension, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, decoder_sequence):\n",
    "        # decoder_sequence: the input sequence to decode (e.g., [START, 1, 2, 3])\n",
    "        # encoder_output: the encoded image from the encoder\n",
    "        # mask: causal mask to prevent attending to future tokens\n",
    "\n",
    "        # First masked self-attention block with residual connection\n",
    "        # This allows decoder sequence to attend to its own past tokens\n",
    "        # First masked self-attention\n",
    "        residual = decoder_sequence\n",
    "        decoder_sequence = self.ln1(decoder_sequence)\n",
    "        decoder_sequence = self.masked_mha(decoder_sequence)\n",
    "        decoder_sequence = residual + decoder_sequence\n",
    "\n",
    "        \n",
    "        # # FFN block with residual connection\n",
    "        residual = decoder_sequence\n",
    "        decoder_sequence = self.ln2(decoder_sequence)\n",
    "        decoder_sequence = self.ffn(decoder_sequence)\n",
    "        decoder_sequence = residual + decoder_sequence\n",
    "        \n",
    "        return decoder_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, mlp_dimension, num_layers, input_sequence_length, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "        # Create positional embeddings for decoder sequence ONCE during initialization\n",
    "        #self.positional_embeddings = nn.Parameter(\n",
    "        #    torch.randn(1, input_sequence_length, embedding_dim),\n",
    "        #    requires_grad=True\n",
    "        #)\n",
    "        \n",
    "        # Create decoder blocks\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderBlock(embedding_dim, num_heads, mlp_dimension)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.final_ln = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Output projection to vocabulary size\n",
    "        # This converts decoder features to logits over possible next tokens\n",
    "        self.output_projection = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, decoder_sequence, return_logits=True):\n",
    "        # decoder_sequence: the input sequence to decode (e.g., [START, 1, 2, 3])\n",
    "        # encoder_output: the encoded image from the encoder\n",
    "        # return_logits: whether to return prediction logits or just decoder features\n",
    "        # by default, we return logits\n",
    "\n",
    "        #embedded_decoder_sequence = self.embedding_layer(decoder_sequence) # not neee because its already embedded\n",
    "\n",
    "        # Add positional embeddings to decoder sequence\n",
    "        #decoder_sequence = embedded_decoder_sequence + self.positional_embeddings\n",
    "        \n",
    "        # Pass through decoder blocks\n",
    "        for block in self.decoder_blocks:\n",
    "            decoder_sequence = block(decoder_sequence)\n",
    "        \n",
    "        # Apply final layer norm\n",
    "        decoder_features = self.final_ln(decoder_sequence)\n",
    "        \n",
    "        if return_logits:\n",
    "            # Convert features to logits for prediction\n",
    "            # Shape: [batch_size, seq_length, vocab_size]\n",
    "            logits = self.output_projection(decoder_features)\n",
    "            return logits\n",
    "        else:\n",
    "            # Return decoder features if needed\n",
    "            return decoder_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n",
      "\n",
      "Image Embeddings shape: torch.Size([10, 49, 768])\n",
      "Text Embeddings shape: torch.Size([10, 32, 512])\n",
      "Target ids shape: torch.Size([10, 32])\n",
      "\n",
      "Projected img shape: torch.Size([10, 49, 512])\n",
      "Projected text shape: torch.Size([10, 32, 512])\n",
      "Text embeddings type: torch.float32\n",
      "Image features type: torch.float32\n",
      "Target ids type: torch.int64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     19\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     21\u001b[39m \n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# Iterate over the DataLoader and print the outputs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mImage Embeddings shape:\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mText Embeddings shape:\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mCaptionDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     34\u001b[39m     text_embeddings = \u001b[38;5;28mself\u001b[39m.clip_model.text_model.embeddings(input_ids_full).squeeze(\u001b[32m0\u001b[39m).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Get the CLIP encoded image embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     patch_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclip_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m.last_hidden_state[:, \u001b[32m1\u001b[39m:, :].squeeze(\u001b[32m0\u001b[39m).to(\u001b[38;5;28mself\u001b[39m.device) \u001b[38;5;66;03m# shape: [1, num_patches, hidden_dim]\u001b[39;00m\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m#print('Patch embeddings shape:', patch_embeddings.shape)           \u001b[39;00m\n\u001b[32m     41\u001b[39m target_ids = input_ids_full.squeeze(\u001b[32m0\u001b[39m).to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:1094\u001b[39m, in \u001b[36mCLIPVisionTransformer.forward\u001b[39m\u001b[34m(self, pixel_values, output_attentions, output_hidden_states, interpolate_pos_encoding)\u001b[39m\n\u001b[32m   1091\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n\u001b[32m   1092\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.pre_layrnorm(hidden_states)\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m encoder_outputs: BaseModelOutput = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1100\u001b[39m last_hidden_state = encoder_outputs.last_hidden_state\n\u001b[32m   1101\u001b[39m pooled_output = last_hidden_state[:, \u001b[32m0\u001b[39m, :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:882\u001b[39m, in \u001b[36mCLIPEncoder.forward\u001b[39m\u001b[34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states)\u001b[39m\n\u001b[32m    874\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    875\u001b[39m         encoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    876\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    879\u001b[39m         output_attentions,\n\u001b[32m    880\u001b[39m     )\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m     layer_outputs = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    889\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    891\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:624\u001b[39m, in \u001b[36mCLIPEncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[39m\n\u001b[32m    622\u001b[39m residual = hidden_states\n\u001b[32m    623\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm2(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    625\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    627\u001b[39m outputs = (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:581\u001b[39m, in \u001b[36mCLIPMLP.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    579\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.fc1(hidden_states)\n\u001b[32m    580\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.activation_fn(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mlx/MLX_week4/mlxw4_image_captioning/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Parameters for decoder\n",
    "embedding_dim = 512\n",
    "num_heads = 8\n",
    "mlp_dimension = 2048\n",
    "num_layers = 2\n",
    "input_sequence_length = 32\n",
    "vocab_size = 49408\n",
    "\n",
    "decoder = Decoder(embedding_dim, num_heads, mlp_dimension, num_layers, input_sequence_length, vocab_size)\n",
    "#decoder = SimpleCaptionDecoder(embedding_dim, num_heads, mlp_dimension, num_layers, vocab_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Iterate over the DataLoader and print the outputs\n",
    "    for patch_embeddings, text_embeddings, target_ids, mask in dataloader:\n",
    "\n",
    "        \n",
    "        print('\\nImage Embeddings shape:', patch_embeddings.shape)\n",
    "        print('Text Embeddings shape:', text_embeddings.shape)\n",
    "\n",
    "        image_input_dim = patch_embeddings.shape[2] # get dimension from patch embeddings\n",
    "        text_embed_dim = text_embeddings.shape[2]\n",
    "        print('Target ids shape:', target_ids.shape)\n",
    "\n",
    "        image_projection_layer = nn.Linear(image_input_dim, text_embed_dim)\n",
    "        img_features = image_projection_layer(patch_embeddings)\n",
    "        print('\\nProjected img shape:', img_features.shape)\n",
    "        print('Projected text shape:', text_embeddings.shape)\n",
    "\n",
    "        # Move to device\n",
    "        encoder_output = img_features.to(device)\n",
    "        decoder_inputs = text_embeddings.to(device).float()\n",
    "        targets = target_ids.to(device)\n",
    "        \n",
    "       \n",
    "        print('Text embeddings type:', decoder_inputs.dtype)\n",
    "        print('Image features type:', encoder_output.dtype)\n",
    "        print('Target ids type:', targets.dtype)\n",
    "\n",
    "        # Forward pass through the decoder\n",
    "        logits = decoder(decoder_inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        # Shift logits and targets to align for cross-entropy\n",
    "        logits = logits[:, :-1].contiguous().view(-1, logits.size(-1))\n",
    "        targets = target_ids[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(logits, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "## Validation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        images, decoder_inputs, targets = batch\n",
    "        outputs = model(images, decoder_inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        print(f\"Validation Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "\n",
    "def train_transformer(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "):\n",
    "    \n",
    "    # Initialize wandb\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_name = f\"full-transformer-{timestamp}\"\n",
    "    wandb.init(project=\"full-transformer-mnist\",\n",
    "               name=run_name,\n",
    "               group=\"experiment-1\",\n",
    "               config={\n",
    "                   \"learning_rate\": learning_rate,\n",
    "                   \"num_epochs\": num_epochs,\n",
    "                   \"batch_size\": train_loader.batch_size,\n",
    "                   \"embedding_dim\": model.embedding_dim,\n",
    "                   \"num_heads\": model.num_heads,\n",
    "                   \"mlp_dimension\": model.mlp_dimension,\n",
    "                   \"num_layers\": model.num_layers,\n",
    "                   \"max_seq_length\": model.num_patches,\n",
    "                   \"vocab_size\": model.vocab_size\n",
    "               })\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # TRAINING LOOP \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Training phase\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', mininterval=1.0) # udpate every 1 second\n",
    "        for batch_idx, (input_images, decoder_inputs, targets) in enumerate(train_loader):\n",
    "            # Move to device\n",
    "            input_images = input_images.to(device)\n",
    "            decoder_inputs = decoder_inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_images, decoder_inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            # outputs.view(-1, outputs.size(-1)) is to flatten the output to a 1D tensor\n",
    "            # targets.view(-1) is to flatten the targets to a 1D tensor\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(-1)\n",
    "            total_correct += (predicted == targets).sum().item()\n",
    "            total_samples += targets.numel()\n",
    "            \n",
    "            # Calculate current accuracy\n",
    "            current_acc = (total_correct / total_samples * 100)\n",
    "            \n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({\n",
    "                'loss': total_loss / (batch_idx + 1),\n",
    "                'acc': f'{current_acc:.2f}%'\n",
    "            })\n",
    "\n",
    "            train_pbar.update(1)\n",
    "        \n",
    "        train_pbar.close()\n",
    "\n",
    "        \n",
    "        # Calculate training metrics after all batches\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = (total_correct / total_samples * 100)\n",
    "        print(f\"\\nTraining completed for epoch {epoch+1}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "        # After training phase\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "            \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "        \n",
    "        # No need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]', mininterval=1.0)\n",
    "            for batch_idx, (images, decoder_input, targets) in enumerate(val_pbar):\n",
    "                # Move to device\n",
    "                images = images.to(device)\n",
    "                decoder_input = decoder_input.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images, decoder_input)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Update validation metrics\n",
    "                _, predicted = outputs.max(-1)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "                val_samples += targets.numel()\n",
    "\n",
    "                # Calculate current validation accuracy\n",
    "                current_val_acc = (val_correct / val_samples * 100)\n",
    "                \n",
    "\n",
    "                # Update progress bar\n",
    "                val_pbar.set_postfix({\n",
    "                    'loss': val_loss / (batch_idx + 1),\n",
    "                    'acc': f'{current_val_acc:.2f}%'\n",
    "                })\n",
    "\n",
    "                val_pbar.update(1)\n",
    "\n",
    "        val_pbar.close()\n",
    "                \n",
    "        # Calculate validation metrics\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = (val_correct / val_samples * 100)\n",
    "\n",
    "        # After validation phase\n",
    "        wandb.log({\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "        \n",
    "\n",
    "      # Print epoch summary\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs} Summary:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "    # After training loop completes\n",
    "    # 1. Save model weights\n",
    "    model_save_path = f\"transformer_weights_{timestamp}_mnistscattered.pt\"\n",
    "\n",
    "    torch.save(model, model_save_path)\n",
    "\n",
    "    print(f\"\\nModel weights saved to {model_save_path}\")\n",
    "\n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Assuming 'data' is one sample from your dataset (e.g., a batch or single entry)\n",
    "image = train['image'][0]  # This will be a PIL.Image object\n",
    "\n",
    "# Check the original image size\n",
    "print(\"Original image size:\", image.size)  # Returns (width, height)\n",
    "\n",
    "# Optionally, check the mode (RGB, grayscale, etc.)\n",
    "print(\"Image mode:\", image.mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions produces a list of captions\n",
    "#captions = [caption for caption_list[0] for caption_list in test['caption']]  \n",
    "\n",
    "captions = []\n",
    "for caption_list in test['caption']:\n",
    "    for caption in caption_list[0]:\n",
    "        captions.append(caption)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, decoder, clip_model_name='openai/clip-vit-base-patch32'):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.clip = CLIPModel.from_pretrained(clip_model_name)\n",
    "        self.clip.visual.requires_grad_(False)  # freeze vision encoder\n",
    "\n",
    "    def forward(self, images, decoder_inputs, tgt_mask=None, tgt_key_padding_mask=None):\n",
    "        # Get image features from CLIP vision encoder\n",
    "        vision_outputs = self.clip.vision_model(pixel_values=images)\n",
    "        img_features = vision_outputs.last_hidden_state.mean(dim=1).unsqueeze(1)  # (B, 1, D)\n",
    "\n",
    "        # Decode using transformer decoder\n",
    "        logits = self.decoder(\n",
    "            memory=img_features,\n",
    "            decoder_inputs=decoder_inputs,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in pbar:\n",
    "        images = batch['images'].squeeze(1).to(device)  # shape (B, C, H, W)\n",
    "        decoder_inputs = batch['decoder_inputs'].to(device)  # (B, T)\n",
    "        decoder_targets = batch['decoder_targets'].to(device)  # (B, T)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images, decoder_inputs)\n",
    "\n",
    "        # Shift logits and targets to align for cross-entropy\n",
    "        logits = logits[:, :-1].contiguous().view(-1, logits.size(-1))\n",
    "        targets = decoder_targets[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageCaptioningModel(decoder=Decoder()).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
