{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPProcessor, CLIPTokenizer, CLIPModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize CLIP processor and tokenizer\n",
    "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n",
    "model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Current device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, dataset, clip_model_name=\"openai/clip-vit-base-patch32\", device=device):\n",
    "        self.image = dataset['image']\n",
    "        self.caption_list = dataset['caption']\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)\n",
    "        self.clip_model = CLIPModel.from_pretrained(clip_model_name).eval().to(self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image[idx]\n",
    "        caption_list = self.caption_list[idx]\n",
    "        \n",
    "        # ---- Encode image with CLIP ----\n",
    "        img_tensor = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "        #print('IMG TENSOR SHAPE', img_tensor.shape) # channels, height, width\n",
    "        \n",
    "        # ---- Tokenize input caption ----\n",
    "        caption = caption_list[0] # get the first caption in the list\n",
    "        #print('caption len:', len(caption))\n",
    "        tokens = self.tokenizer(caption, padding=\"max_length\", max_length=32, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "        input_ids_full = tokens[\"input_ids\"].to(self.device)  # [1, seq_len]\n",
    "        #print('text_input_ids_full shape:', input_ids_full.shape)\n",
    "        mask = tokens[\"attention_mask\"].to(self.device) # get the mask out\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Use only embedding layer from CLIP\n",
    "            text_embeddings = self.clip_model.text_model.embeddings(input_ids_full).squeeze(0).to(self.device)\n",
    "\n",
    "            # Get the CLIP encoded image embeddings\n",
    "            patch_embeddings = self.clip_model.vision_model(**img_tensor).last_hidden_state[:, 1:, :].squeeze(0).to(self.device) # shape: [1, num_patches, hidden_dim]\n",
    "            #print('Patch embeddings shape:', patch_embeddings.shape)           \n",
    "            \n",
    "            \n",
    "        target_ids = input_ids_full.squeeze(0).to(self.device)\n",
    "\n",
    "        #print('IMG EMBEDDINGS SHAPE', patch_embeddings.shape)\n",
    "        #print('TEXT EMBEDDINGS SHAPE', text_embeddings.shape)\n",
    "        #print('TARGET IDS SHAPE', target_ids.shape)\n",
    "        #print('MASK SHAPE', mask.shape)\n",
    "\n",
    "        return patch_embeddings, text_embeddings, target_ids, mask\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Iterate over the DataLoader and print the outputs\n",
    "for batch_idx, (patch_embeddings, text_embeddings, target_ids, mask) in enumerate(test_dataloader):\n",
    "    print('\\nImage Embeddings shape:', patch_embeddings.shape)\n",
    "    print('Text Embeddings shape:', text_embeddings.shape)\n",
    "\n",
    "    # Move all to device\n",
    "    patch_embeddings = patch_embeddings.to(device)\n",
    "    text_embeddings = text_embeddings.to(device)\n",
    "    target_ids = target_ids.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    image_input_dim = patch_embeddings.shape[2]# get dimension from patch embeddings\n",
    "    text_embed_dim = text_embeddings.shape[2]\n",
    "\n",
    "    # Move projection layer to device\n",
    "    image_projection_layer = nn.Linear(image_input_dim, text_embed_dim).to(device)\n",
    "    img_features = image_projection_layer(patch_embeddings)\n",
    "    print('\\nProjected img shape:', img_features.shape)\n",
    "    print('Projected text shape:', text_embeddings.shape)\n",
    "\n",
    "    print('Text tensor type:', text_embeddings.dtype)\n",
    "    print('Image tensor type:', img_features.dtype)\n",
    "    break  # Remove this break to iterate over the entire subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create the image captioning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬆️ The code above projects the text and image embeddings into the same shape so now we can send it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Masked Self Attention Head\n",
    "class MaskedAttentionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_dim):\n",
    "        super(MaskedAttentionHead, self).__init__()\n",
    "        self.head_dim = head_dim\n",
    "\n",
    "        # Linear projections for query, key, value\n",
    "        self.weight_q = nn.Linear(embedding_dim, head_dim)\n",
    "        self.weight_k = nn.Linear(embedding_dim, head_dim)\n",
    "        self.weight_v = nn.Linear(embedding_dim, head_dim)\n",
    "\n",
    "        self.linear_projection = nn.Linear(head_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, decoder_sequence, input_sequence_length, padding_mask=None, ):\n",
    "        # embedded decoder sequence shape: [batch_size, seq_length, embedding_dim]\n",
    "\n",
    "        #print('Decoder sequence shape:', decoder_sequence.shape)\n",
    "\n",
    "        # Project to head dimension\n",
    "        Q = self.weight_q(decoder_sequence)\n",
    "        K = self.weight_k(decoder_sequence)\n",
    "        V = self.weight_v(decoder_sequence)\n",
    "\n",
    "        # Make the mask\n",
    "        decoder_sequence_length = decoder_sequence.shape[1]\n",
    "        #mask = torch.triu(torch.ones(decoder_sequence_length, decoder_sequence_length, device=decoder_sequence.device), diagonal=1)\n",
    "        #print(\"Causal mask shape:\", mask.shape)\n",
    "        #mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "\n",
    "        mask = torch.zeros(decoder_sequence_length, decoder_sequence_length, device=decoder_sequence.device)\n",
    "        \n",
    "        # Only mask the text portion (after image patches)\n",
    "        num_image_patches = 49  # CLIP ViT-B/32 has 49 patches\n",
    "        text_start_idx = num_image_patches\n",
    "        \n",
    "        # Create causal mask for text portion only\n",
    "        text_mask = torch.triu(torch.ones(decoder_sequence_length - text_start_idx, decoder_sequence_length - text_start_idx, \n",
    "                                        device=decoder_sequence.device), diagonal=1)\n",
    "        text_mask = text_mask.masked_fill(text_mask==1, float('-inf'))\n",
    "        \n",
    "        # Place the text mask in the bottom-right corner of the full mask\n",
    "        mask[text_start_idx:, text_start_idx:] = text_mask\n",
    "        \n",
    "\n",
    "        # Calculate attention scores (scaled dot product)\n",
    "        A = torch.einsum('bid,bjd->bij', Q, K)\n",
    "        A = A / (self.head_dim ** 0.5) \n",
    "\n",
    "        A = A + mask\n",
    "\n",
    "        \n",
    "        # Apply padding mask if provided\n",
    "        #if padding_mask is not None:\n",
    "            #print('Padding mask shape:', padding_mask.shape)\n",
    "            # Convert padding mask to attention mask\n",
    "        #    padding_mask = padding_mask.expand(-1, A.size(1), -1)\n",
    "        #    A = A.masked_fill(padding_mask, float('-inf'))\n",
    "            \n",
    "        # Apply softmax\n",
    "        A = torch.softmax(A, dim=-1)\n",
    "\n",
    "    \n",
    "        #  Apply attention weights to values\n",
    "        H = torch.einsum('bij,bjd->bid', A, V)\n",
    "        \n",
    "        # Add projection layer for output to return back to the original embedding dimension\n",
    "        #output = self.linear_projection(H)\n",
    "\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "\n",
    "     \n",
    "        self.heads = nn.ModuleList(\n",
    "            [MaskedAttentionHead(embedding_dim, self.head_dim) for _ in range(num_heads)]\n",
    "            )\n",
    "        \n",
    "        # The output of the CrossAttention Head and MaskedAttentionHead still needs to be projected\n",
    "        # Back to the embedding dimensions of the head_dim x vocab_size\n",
    "        \n",
    "        self.output_projection = nn.Linear(num_heads * self.head_dim, embedding_dim)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, decoder_sequence, padding_mask=None):\n",
    "        # decoder_sequence: [batch_size, seq_length, embedding_dim]\n",
    "        # encoder_output: [batch_size, num_patches, embedding_dim] (only used in cross-attention)\n",
    "        # mask: [batch_size, seq_length, seq_length] (only used in self-attention)\n",
    "\n",
    "        # Process each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "                # For masked self-attention, we only need decoder sequence and mask\n",
    "                head_output = head(decoder_sequence, padding_mask)\n",
    "                #print(\"\\nmasked attention head output shape: \", head_output.shape)\n",
    "                \n",
    "                head_outputs.append(head_output)\n",
    "\n",
    "        # Concatenate head outputs\n",
    "        concat_heads = torch.cat(head_outputs, dim=-1)\n",
    "        \n",
    "        # Project back to embedding dimension\n",
    "        output = self.output_projection(concat_heads)\n",
    "        #print(\"Multihead attention output shape: \", output.shape)\n",
    "        \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, mlp_dimension):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        \n",
    "        # First layer norm\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # Masked multi-head attention for decoder sequence self-attention\n",
    "        self.masked_mha = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        \n",
    "        \n",
    "        # Third layer norm\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # Feed forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, mlp_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_dimension, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, decoder_sequence, padding_mask=None):\n",
    "        # decoder_sequence: the input sequence to decode (e.g., [START, 1, 2, 3])\n",
    "        # encoder_output: the encoded image from the encoder\n",
    "        # mask: causal mask to prevent attending to future tokens\n",
    "\n",
    "        # First masked self-attention block with residual connection\n",
    "        # This allows decoder sequence to attend to its own past tokens\n",
    "        # First masked self-attention\n",
    "        residual = decoder_sequence\n",
    "        decoder_sequence = self.ln1(decoder_sequence)\n",
    "        decoder_sequence = self.masked_mha(decoder_sequence, padding_mask)\n",
    "        decoder_sequence = residual + decoder_sequence\n",
    "\n",
    "        \n",
    "        # # FFN block with residual connection\n",
    "        residual = decoder_sequence\n",
    "        decoder_sequence = self.ln2(decoder_sequence)\n",
    "        decoder_sequence = self.ffn(decoder_sequence)\n",
    "        decoder_sequence = residual + decoder_sequence\n",
    "        \n",
    "        return decoder_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, mlp_dimension, num_layers, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "        # Create positional embeddings for decoder sequence ONCE during initialization\n",
    "        #self.positional_embeddings = nn.Parameter(\n",
    "        #    torch.randn(1, 1, embedding_dim), # 81 because its 32 caption len + 49 patches\n",
    "        #    requires_grad=True\n",
    "       # )\n",
    "        \n",
    "        # Create decoder blocks\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderBlock(embedding_dim, num_heads, mlp_dimension)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.final_ln = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Output projection to vocabulary size\n",
    "        # This converts decoder features to logits over possible next tokensß\n",
    "        self.output_projection = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, text_embeddings, img_features, padding_mask=None, return_logits=True):\n",
    "        # return_logits: whether to return prediction logits or just decoder features\n",
    "        # by default, we return logits\n",
    "\n",
    "        # Add positional embeddings to decoder sequence\n",
    "        #decoder_sequence = embedded_decoder_sequence + self.positional_embeddings\n",
    "        \n",
    "        # Concatenate image features and text embeddings\n",
    "        decoder_inputs = torch.cat([img_features, text_embeddings], dim=1).to(device)\n",
    "        #print('Decoder inputs shape:', decoder_inputs.shape)\n",
    "        \n",
    "        # Add new positional embeddings\n",
    "        #decoder_inputs = decoder_inputs + self.positional_embeddings\n",
    "\n",
    "        # Pass through decoder blocks\n",
    "        for block in self.decoder_blocks:\n",
    "            decoder_inputs = block(decoder_inputs, padding_mask)\n",
    "        \n",
    "        # Apply final layer norm\n",
    "        decoder_features = self.final_ln(decoder_inputs)\n",
    "\n",
    "        # Get text hidden states only\n",
    "        text_hidden = decoder_features[:, -32:, :]\n",
    "        \n",
    "        if return_logits:\n",
    "            # Convert features to logits for prediction\n",
    "            # Shape: [batch_size, seq_length, vocab_size]\n",
    "            logits = self.output_projection(text_hidden)\n",
    "            return logits\n",
    "        else:\n",
    "            # Return decoder features if needed\n",
    "            return text_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
      "    num_rows: 1500\n",
      "})\n",
      "Dataset({\n",
      "    features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
      "    num_rows: 3500\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# load dataset and create train and test sets\n",
    "raw_dataset = load_dataset(\"nlphuji/flickr30k\", split='test[:5000]')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_test_split = raw_dataset.train_test_split(test_size=0.3)\n",
    "train = train_test_split['train']\n",
    "test = train_test_split['test']\n",
    "\n",
    "print(test)\n",
    "print(train)\n",
    "\n",
    "test_caption = CaptionDataset(test)\n",
    "train_caption = CaptionDataset(train)\n",
    "\n",
    "# Create a DataLoader\n",
    "test_dataloader = DataLoader(test_caption, batch_size=10, shuffle=False)  # Adjust batch_size as needed\n",
    "train_dataloader = DataLoader(train_caption, batch_size=10, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 350/350 [00:58<00:00,  5.98it/s, batch_loss=4.5584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 5.6387\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Val]: 100%|██████████| 150/150 [00:18<00:00,  8.33it/s, val_loss=4.5031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Val Loss: 4.6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 350/350 [00:57<00:00,  6.05it/s, batch_loss=3.6821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 4.1242\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Val]: 100%|██████████| 150/150 [00:17<00:00,  8.50it/s, val_loss=3.8685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Val Loss: 4.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 350/350 [00:57<00:00,  6.05it/s, batch_loss=3.1266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 3.4573\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Val]: 100%|██████████| 150/150 [00:17<00:00,  8.50it/s, val_loss=3.3747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Val Loss: 3.5670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 350/350 [00:57<00:00,  6.07it/s, batch_loss=2.5849]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 2.9215\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Val]: 100%|██████████| 150/150 [00:17<00:00,  8.43it/s, val_loss=2.9678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Val Loss: 3.2224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 350/350 [00:57<00:00,  6.07it/s, batch_loss=1.9813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 2.4050\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Val]: 100%|██████████| 150/150 [00:17<00:00,  8.52it/s, val_loss=2.6680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Val Loss: 2.9383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 350/350 [00:57<00:00,  6.13it/s, batch_loss=1.5471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 1.9464\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Val]: 100%|██████████| 150/150 [00:17<00:00,  8.38it/s, val_loss=2.4097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Val Loss: 2.6949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 350/350 [00:57<00:00,  6.11it/s, batch_loss=1.1802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 1.5434\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Val]: 100%|██████████| 150/150 [00:17<00:00,  8.48it/s, val_loss=2.2654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 Val Loss: 2.4974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 350/350 [00:57<00:00,  6.07it/s, batch_loss=0.8399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 1.1833\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Val]: 100%|██████████| 150/150 [00:17<00:00,  8.45it/s, val_loss=2.1040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 Val Loss: 2.3865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 350/350 [00:57<00:00,  6.12it/s, batch_loss=0.5723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.8636\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Val]: 100%|██████████| 150/150 [00:17<00:00,  8.43it/s, val_loss=2.0207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 Val Loss: 2.2935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 350/350 [00:56<00:00,  6.19it/s, batch_loss=0.3785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.5861\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Val]: 100%|██████████| 150/150 [00:17<00:00,  8.51it/s, val_loss=1.9106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 Val Loss: 2.2954\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Parameters for decoder\n",
    "embedding_dim = 512\n",
    "num_heads = 8\n",
    "mlp_dimension = 2048\n",
    "num_layers = 6\n",
    "input_sequence_length = 32\n",
    "vocab_size = 49408\n",
    "\n",
    "decoder = Decoder(embedding_dim=embedding_dim, num_heads=num_heads, \n",
    "                  mlp_dimension=mlp_dimension, num_layers=num_layers, \n",
    "                  vocab_size=vocab_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=1e-4)\n",
    "\n",
    "# Move projection layer to device\n",
    "image_projection_layer = nn.Linear(768, embedding_dim).to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    decoder.train()\n",
    "\n",
    "    # Training phase\n",
    "    train_pbar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', mininterval=0.1)\n",
    "\n",
    "    epoch_losses = [] # store all individual losses and calculate the true average at the end\n",
    "\n",
    "    # Iterate over the DataLoader and print the outputs\n",
    "    for batch_idx, (patch_embeddings, text_embeddings, target_ids, mask) in enumerate(train_pbar):\n",
    "\n",
    "        # Move everything to device\n",
    "        patch_embeddings = patch_embeddings.to(device)\n",
    "        text_embeddings = text_embeddings.to(device)\n",
    "        mask = mask.to(device)\n",
    "        targets = target_ids.to(device)\n",
    "        img_features = image_projection_layer(patch_embeddings)\n",
    "\n",
    "    \n",
    "        #print('\\nProjected img shape:', img_features.shape)\n",
    "        #print('Projected text shape:', text_embeddings.shape)\n",
    "\n",
    "        #print('Mask shape:', mask.shape)\n",
    "\n",
    "\n",
    "        text_embeddings = text_embeddings.to(device)\n",
    "        img_features = img_features.to(device)\n",
    "        targets = target_ids.to(device)\n",
    "\n",
    "        # Forward pass through the decoder\n",
    "        logits = decoder(text_embeddings, img_features)\n",
    "\n",
    "        #print('logits shape:', logits.shape)\n",
    "\n",
    "        # Compute loss\n",
    "        # Shift logits and targets to align for cross-entropy\n",
    "        logits = logits[:, :-1].contiguous().view(-1, logits.size(-1))\n",
    "        targets = target_ids[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(logits, targets)\n",
    "\n",
    "        # Calculate loss only on text portion\n",
    "        #text_logits = logits[:, -32:, :] # Shape: [batch_size, 32, vocab_size]\n",
    "        #print('text_logits shape:', text_logits.shape)\n",
    "\n",
    "        # Sqeuueze mask to remove th eextra dimension\n",
    "        #mask=mask.squeeze(1)\n",
    "        #print('mask shape:', mask.shape)\n",
    "\n",
    "        # Apply teacher forcing to text portion only \n",
    "        #text_logits = text_logits[:, :-1].contiguous().view(-1, text_logits.size(-1))\n",
    "        #text_targets = target_ids[:, 1:].contiguous().view(-1)\n",
    "        #text_mask = mask[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        #print('text_logits shape:', text_logits.shape)\n",
    "        #print('text_targets shape:', text_targets.shape)\n",
    "       \n",
    "        # Calculate loss only on masked positions\n",
    "        #text_mask = text_mask.bool()\n",
    "        #loss = criterion(\n",
    "        #    text_logits[text_mask],\n",
    "        #    text_targets[text_mask]\n",
    "        #)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Store the loss for this batch\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        # Update progress bar with current batch loss\n",
    "        train_pbar.set_postfix({\n",
    "            'batch_loss': f'{loss.item():.4f}'\n",
    "        })\n",
    "        train_pbar.update(1)\n",
    "\n",
    "    # Close progress bar for this epoch\n",
    "    train_pbar.close()\n",
    "    \n",
    "    # Print epoch summary\n",
    "    # Calculate and print training epoch average loss\n",
    "    avg_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(\" \")\n",
    "\n",
    "     # Validation phase\n",
    "    decoder.eval()  # Set model to evaluation mode\n",
    "    val_losses = []  # Store all validation losses\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        val_pbar = tqdm(test_dataloader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]', mininterval=0.1)\n",
    "        \n",
    "        for batch_idx, (patch_embeddings, text_embeddings, target_ids, mask) in enumerate(val_pbar):\n",
    "\n",
    "            # Move everything to device\n",
    "            patch_embeddings = patch_embeddings.to(device)\n",
    "            text_embeddings = text_embeddings.to(device)\n",
    "            mask = mask.to(device)\n",
    "            targets = target_ids.to(device)\n",
    "            img_features = image_projection_layer(patch_embeddings)\n",
    "\n",
    "\n",
    "            # Forward pass through the decoder\n",
    "            logits = decoder(text_embeddings, img_features)\n",
    "\n",
    "            # Compute loss\n",
    "            # Shift logits and targets to align for cross-entropy\n",
    "            logits = logits[:, :-1].contiguous().view(-1, logits.size(-1))\n",
    "            targets = target_ids[:, 1:].contiguous().view(-1)\n",
    "            val_loss = criterion(logits, targets)\n",
    "\n",
    "            # Calculate loss only on text portion\n",
    "            #text_logits = logits[:, -32:, :] # Shape: [batch_size, 32, vocab_size]\n",
    "            #print('text_logits shape:', text_logits.shape)\n",
    "\n",
    "            # Sqeuueze mask to remove th eextra dimension\n",
    "            #mask=mask.squeeze(1)\n",
    "            #print('mask shape:', mask.shape)\n",
    "\n",
    "            # Apply teacher forcing to text portion only \n",
    "            #text_logits = text_logits[:, :-1].contiguous().view(-1, text_logits.size(-1))\n",
    "            #text_targets = target_ids[:, 1:].contiguous().view(-1)\n",
    "            #text_mask = mask[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            #print('text_logits shape:', text_logits.shape)\n",
    "            #print('text_targets shape:', text_targets.shape)\n",
    "        \n",
    "\n",
    "            # Calculate loss only on masked positions\n",
    "            #text_mask = text_mask.bool()\n",
    "            #val_loss = criterion(\n",
    "            #    text_logits[text_mask],\n",
    "            #    text_targets[text_mask]\n",
    "            #)\n",
    "\n",
    "            # Store validation loss\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Update progress bar with current validation loss\n",
    "            val_pbar.set_postfix({\n",
    "                'val_loss': f'{val_loss.item():.4f}'\n",
    "            })\n",
    "            val_pbar.update(1)\n",
    "    \n",
    "    # Close validation progress bar\n",
    "    val_pbar.close()\n",
    "    \n",
    "    # Calculate average validation loss\n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        \n",
    "print(\"Training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to decoder_08051755.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the model using torch load\n",
    "model_save_path = \"decoder_08051755.pt\"\n",
    "torch.save(decoder.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import CLIPModel, CLIPProcessor, CLIPTokenizer\n",
    "\n",
    "# Ensure device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Current device:', device)\n",
    "\n",
    "# Example usage:\n",
    "# Load models and processors\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Initialize decoder\n",
    "embedding_dim = 512\n",
    "num_heads = 8\n",
    "mlp_dimension = 2048\n",
    "num_layers = 2\n",
    "vocab_size = 49408\n",
    "\n",
    "decoder = Decoder(embedding_dim, num_heads, mlp_dimension, num_layers, vocab_size).to(device)\n",
    "\n",
    "# Load trained weights if provided\n",
    "decoder = torch.load(\"decoder_08051505.pt\", weights_only=False)\n",
    "\n",
    "decoder.eval()\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "# Get a random image from the test set\n",
    "idx = random.randint(0, len(test) - 1)\n",
    "sample = test[idx]\n",
    "image = sample['image']\n",
    "true_caption = sample['caption'][0]  # Get the first caption\n",
    "\n",
    "# Define the generate_caption function\n",
    "def generate_caption(embedding_dim, image, decoder, clip_model, clip_processor, tokenizer, device, max_length=32):\n",
    "    # Process image through CLIP\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get image embeddings\n",
    "        patch_embeddings = clip_model.vision_model(**inputs).last_hidden_state[:, 1:, :].squeeze(0)\n",
    "        \n",
    "        # Project image features\n",
    "        image_projection_layer = torch.nn.Linear(patch_embeddings.shape[-1], embedding_dim).to(device)\n",
    "        img_features = image_projection_layer(patch_embeddings)\n",
    "        \n",
    "        # Add batch dimension to img_features\n",
    "        img_features = img_features.unsqueeze(0)  # Shape: [1, num_patches, embedding_dim]\n",
    "    \n",
    "    # Initialize with start token\n",
    "    start_token = tokenizer.encode(\"<|startoftext|>\")[0]\n",
    "    end_token = tokenizer.encode(\"<|endoftext|>\")[0]\n",
    "    \n",
    "    generated_tokens = [start_token]\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for _ in range(max_length):\n",
    "        # Convert generated tokens to tensor\n",
    "        input_ids = torch.tensor([generated_tokens]).to(device)\n",
    "        \n",
    "        # Get text embeddings\n",
    "        with torch.no_grad():\n",
    "            text_embeddings = clip_model.text_model.embeddings(input_ids).squeeze(0)\n",
    "            # Add batch dimension to text_embeddings\n",
    "            text_embeddings = text_embeddings.unsqueeze(0)  # Shape: [1, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get logits from decoder\n",
    "        logits = decoder(text_embeddings, img_features)\n",
    "\n",
    "        # Get the probabilities of the next token\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get next token (only look at the last position)\n",
    "        next_token = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        generated_tokens.append(next_token)\n",
    "        \n",
    "        # Stop if we generate the end token\n",
    "        if next_token == end_token:\n",
    "            break\n",
    "            \n",
    "    # Decode the generated tokens to a caption\n",
    "    generated_caption = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return generated_caption\n",
    "\n",
    "# Generate caption using the trained decoder\n",
    "generated_caption = generate_caption(embedding_dim, image, decoder, clip_model, clip_processor, tokenizer, device)\n",
    "\n",
    "# Display image with both captions\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Print captions separately for better readability\n",
    "print(f\"Generated Caption: {generated_caption}\")\n",
    "print(f\"Target Caption: {true_caption}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DUMP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Assuming 'data' is one sample from your dataset (e.g., a batch or single entry)\n",
    "image = train['image'][0]  # This will be a PIL.Image object\n",
    "\n",
    "# Check the original image size\n",
    "print(\"Original image size:\", image.size)  # Returns (width, height)\n",
    "\n",
    "# Optionally, check the mode (RGB, grayscale, etc.)\n",
    "print(\"Image mode:\", image.mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions produces a list of captions\n",
    "#captions = [caption for caption_list[0] for caption_list in test['caption']]  \n",
    "\n",
    "captions = []\n",
    "for caption_list in test['caption']:\n",
    "    for caption in caption_list[0]:\n",
    "        captions.append(caption)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change from total_loss and num_batches to epoch_losses = [] offers several advantages:\n",
    "More Accurate Average:\n",
    "With total_loss and num_batches, you're calculating a running average that might be affected by the order of batches\n",
    "With epoch_losses, you store all individual losses and calculate the true average at the end, which is more accurate\n",
    "More Information:\n",
    "By storing all losses in a list, you can:\n",
    "Calculate the median loss\n",
    "Find the minimum and maximum losses\n",
    "Plot the loss distribution\n",
    "Detect outliers or unusual patterns\n",
    "Calculate standard deviation\n",
    "Debugging and Analysis:\n",
    "If you notice something unusual in the average loss, you can look at the individual losses to understand what's happening\n",
    "You can track how the loss changes throughout the epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
