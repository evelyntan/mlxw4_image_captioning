{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPProcessor, CLIPTokenizer, CLIPModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize CLIP processor and tokenizer\n",
    "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n",
    "model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Current device:', device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Optionally, collect inter-process memory that is no longer needed\n",
    "torch.cuda.ipc_collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, dataset, clip_model_name=\"openai/clip-vit-base-patch32\", device=device):\n",
    "        self.image = dataset['image']\n",
    "        self.caption_list = dataset['caption']\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)\n",
    "        self.clip_model = CLIPModel.from_pretrained(clip_model_name).eval().to(self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image[idx]\n",
    "        caption_list = self.caption_list[idx]\n",
    "        \n",
    "        # ---- Encode image with CLIP ----\n",
    "        img_tensor = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "        #print('IMG TENSOR SHAPE', img_tensor.shape) # channels, height, width\n",
    "        \n",
    "        # ---- Tokenize input caption ----\n",
    "        caption = caption_list[0] # get the first caption in the list\n",
    "        #print('caption len:', len(caption))\n",
    "        tokens = self.tokenizer(caption, padding=\"max_length\", max_length=32, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "        input_ids_full = tokens[\"input_ids\"].to(self.device)  # [1, seq_len]\n",
    "        #print('text_input_ids_full shape:', input_ids_full.shape)\n",
    "        mask = tokens[\"attention_mask\"].to(self.device) # get the mask out\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Use only embedding layer from CLIP\n",
    "            text_embeddings = self.clip_model.text_model.embeddings(input_ids_full).squeeze(0).to(self.device)\n",
    "\n",
    "            # Get the CLIP encoded image embeddings\n",
    "            patch_embeddings = self.clip_model.vision_model(**img_tensor).last_hidden_state[:, 1:, :].squeeze(0).to(self.device) # shape: [1, num_patches, hidden_dim]\n",
    "            #print('Patch embeddings shape:', patch_embeddings.shape)           \n",
    "            \n",
    "            \n",
    "        target_ids = input_ids_full.squeeze(0).to(self.device)\n",
    "\n",
    "        #print('IMG EMBEDDINGS SHAPE', patch_embeddings.shape)\n",
    "        #print('TEXT EMBEDDINGS SHAPE', text_embeddings.shape)\n",
    "        #print('TARGET IDS SHAPE', target_ids.shape)\n",
    "        #print('MASK SHAPE', mask.shape)\n",
    "\n",
    "        return patch_embeddings, text_embeddings, target_ids, mask\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Iterate over the DataLoader and print the outputs\n",
    "for batch_idx, (patch_embeddings, text_embeddings, target_ids, mask) in enumerate(test_dataloader):\n",
    "    print('\\nImage Embeddings shape:', patch_embeddings.shape)\n",
    "    print('Text Embeddings shape:', text_embeddings.shape)\n",
    "\n",
    "    # Move all to device\n",
    "    patch_embeddings = patch_embeddings.to(device)\n",
    "    text_embeddings = text_embeddings.to(device)\n",
    "    target_ids = target_ids.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    image_input_dim = patch_embeddings.shape[2]# get dimension from patch embeddings\n",
    "    text_embed_dim = text_embeddings.shape[2]\n",
    "\n",
    "    # Move projection layer to device\n",
    "    image_projection_layer = nn.Linear(image_input_dim, text_embed_dim).to(device)\n",
    "    img_features = image_projection_layer(patch_embeddings)\n",
    "    print('\\nProjected img shape:', img_features.shape)\n",
    "    print('Projected text shape:', text_embeddings.shape)\n",
    "\n",
    "    print('Text tensor type:', text_embeddings.dtype)\n",
    "    print('Image tensor type:', img_features.dtype)\n",
    "    break  # Remove this break to iterate over the entire subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create the image captioning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬆️ The code above projects the text and image embeddings into the same shape so now we can send it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Masked Self Attention Head\n",
    "class MaskedAttentionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_dim):\n",
    "        super(MaskedAttentionHead, self).__init__()\n",
    "        self.head_dim = head_dim\n",
    "\n",
    "        # Linear projections for query, key, value\n",
    "        self.weight_q = nn.Linear(embedding_dim, head_dim)\n",
    "        self.weight_k = nn.Linear(embedding_dim, head_dim)\n",
    "        self.weight_v = nn.Linear(embedding_dim, head_dim)\n",
    "\n",
    "        self.linear_projection = nn.Linear(head_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, decoder_sequence):\n",
    "        # embedded decoder sequence shape: [batch_size, seq_length, embedding_dim]\n",
    "\n",
    "        # Project to head dimension\n",
    "        Q = self.weight_q(decoder_sequence)\n",
    "        K = self.weight_k(decoder_sequence)\n",
    "        V = self.weight_v(decoder_sequence)\n",
    "\n",
    "        # Make the mask\n",
    "        seq_len = decoder_sequence.shape[1]\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=decoder_sequence.device), diagonal=1)\n",
    "        mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "\n",
    "        # Calculate attention scores (scaled dot product)\n",
    "        A = torch.einsum('bid,bjd->bij', Q, K)\n",
    "        A = A / (self.head_dim ** 0.5) \n",
    "\n",
    "        A = A + mask\n",
    "        # Apply softmax\n",
    "        A = torch.softmax(A, dim=-1)\n",
    "\n",
    "    \n",
    "        #  Apply attention weights to values\n",
    "        H = torch.einsum('bij,bjd->bid', A, V)\n",
    "        \n",
    "        # Add projection layer for output to return back to the original embedding dimension\n",
    "        #output = self.linear_projection(H)\n",
    "\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "\n",
    "     \n",
    "        self.heads = nn.ModuleList(\n",
    "            [MaskedAttentionHead(embedding_dim, self.head_dim) for _ in range(num_heads)]\n",
    "            )\n",
    "        \n",
    "        # The output of the CrossAttention Head and MaskedAttentionHead still needs to be projected\n",
    "        # Back to the embedding dimensions of the head_dim x vocab_size\n",
    "        \n",
    "        self.output_projection = nn.Linear(num_heads * self.head_dim, embedding_dim)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, decoder_sequence):\n",
    "        # decoder_sequence: [batch_size, seq_length, embedding_dim]\n",
    "        # encoder_output: [batch_size, num_patches, embedding_dim] (only used in cross-attention)\n",
    "        # mask: [batch_size, seq_length, seq_length] (only used in self-attention)\n",
    "\n",
    "        # Process each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "                # For masked self-attention, we only need decoder sequence and mask\n",
    "                head_output = head(decoder_sequence)\n",
    "                #print(\"\\nmasked attention head output shape: \", head_output.shape)\n",
    "                \n",
    "                head_outputs.append(head_output)\n",
    "\n",
    "        # Concatenate head outputs\n",
    "        concat_heads = torch.cat(head_outputs, dim=-1)\n",
    "        \n",
    "        # Project back to embedding dimension\n",
    "        output = self.output_projection(concat_heads)\n",
    "        #print(\"Multihead attention output shape: \", output.shape)\n",
    "        \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, mlp_dimension):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        \n",
    "        # First layer norm\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # Masked multi-head attention for decoder sequence self-attention\n",
    "        self.masked_mha = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        \n",
    "        \n",
    "        # Third layer norm\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # Feed forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, mlp_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_dimension, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, decoder_sequence):\n",
    "        # decoder_sequence: the input sequence to decode (e.g., [START, 1, 2, 3])\n",
    "        # encoder_output: the encoded image from the encoder\n",
    "        # mask: causal mask to prevent attending to future tokens\n",
    "\n",
    "        # First masked self-attention block with residual connection\n",
    "        # This allows decoder sequence to attend to its own past tokens\n",
    "        # First masked self-attention\n",
    "        residual = decoder_sequence\n",
    "        decoder_sequence = self.ln1(decoder_sequence)\n",
    "        decoder_sequence = self.masked_mha(decoder_sequence)\n",
    "        decoder_sequence = residual + decoder_sequence\n",
    "\n",
    "        \n",
    "        # # FFN block with residual connection\n",
    "        residual = decoder_sequence\n",
    "        decoder_sequence = self.ln2(decoder_sequence)\n",
    "        decoder_sequence = self.ffn(decoder_sequence)\n",
    "        decoder_sequence = residual + decoder_sequence\n",
    "        \n",
    "        return decoder_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, mlp_dimension, num_layers, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "        # Create positional embeddings for decoder sequence ONCE during initialization\n",
    "        #self.positional_embeddings = nn.Parameter(\n",
    "        #    torch.randn(1, input_sequence_length, embedding_dim),\n",
    "        #    requires_grad=True\n",
    "        #)\n",
    "        \n",
    "        # Create decoder blocks\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderBlock(embedding_dim, num_heads, mlp_dimension)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.final_ln = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Output projection to vocabulary size\n",
    "        # This converts decoder features to logits over possible next tokens\n",
    "        self.output_projection = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, concatenated_embeddings, return_logits=True):\n",
    "        # return_logits: whether to return prediction logits or just decoder features\n",
    "        # by default, we return logits\n",
    "\n",
    "        # Add positional embeddings to decoder sequence\n",
    "        #decoder_sequence = embedded_decoder_sequence + self.positional_embeddings\n",
    "        \n",
    "        # Pass through decoder blocks\n",
    "        for block in self.decoder_blocks:\n",
    "            concatenated_embeddings = block(concatenated_embeddings)\n",
    "        \n",
    "        # Apply final layer norm\n",
    "        decoder_features = self.final_ln(concatenated_embeddings)\n",
    "        \n",
    "        if return_logits:\n",
    "            # Convert features to logits for prediction\n",
    "            # Shape: [batch_size, seq_length, vocab_size]\n",
    "            logits = self.output_projection(decoder_features)\n",
    "            return logits\n",
    "        else:\n",
    "            # Return decoder features if needed\n",
    "            return decoder_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
      "    num_rows: 1500\n",
      "})\n",
      "Dataset({\n",
      "    features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
      "    num_rows: 3500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# load dataset and create train and test sets\n",
    "raw_dataset = load_dataset(\"nlphuji/flickr30k\", split='test[:5000]')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_test_split = raw_dataset.train_test_split(test_size=0.3)\n",
    "train = train_test_split['train']\n",
    "test = train_test_split['test']\n",
    "\n",
    "print(test)\n",
    "print(train)\n",
    "\n",
    "test_caption = CaptionDataset(test)\n",
    "train_caption = CaptionDataset(train)\n",
    "\n",
    "# Create a DataLoader\n",
    "test_dataloader = DataLoader(test_caption, batch_size=50, shuffle=False)  # Adjust batch_size as needed\n",
    "train_dataloader = DataLoader(train_caption, batch_size=50, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|██████████| 70/70 [00:42<00:00,  1.64it/s, batch_loss=5.4321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 7.5467\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Val]: 100%|██████████| 30/30 [00:15<00:00,  1.94it/s, val_loss=5.6531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Val Loss: 5.4848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 70/70 [00:43<00:00,  1.61it/s, batch_loss=3.8824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Train Loss: 4.6026\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Val]: 100%|██████████| 30/30 [00:15<00:00,  1.98it/s, val_loss=4.2237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 Val Loss: 4.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]: 100%|██████████| 70/70 [00:41<00:00,  1.68it/s, batch_loss=2.9855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Train Loss: 3.5239\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Val]: 100%|██████████| 30/30 [00:14<00:00,  2.00it/s, val_loss=3.3860]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 Val Loss: 3.1297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 70/70 [00:42<00:00,  1.64it/s, batch_loss=2.3364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Train Loss: 2.7366\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Val]: 100%|██████████| 30/30 [00:15<00:00,  2.00it/s, val_loss=2.8021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 Val Loss: 2.4917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Train]: 100%|██████████| 70/70 [00:41<00:00,  1.68it/s, batch_loss=1.9062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 2.2030\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Val]: 100%|██████████| 30/30 [00:15<00:00,  1.99it/s, val_loss=2.3559]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 Val Loss: 2.0748\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parameters for decoder\n",
    "embedding_dim = 512\n",
    "num_heads = 8\n",
    "mlp_dimension = 2048\n",
    "num_layers = 2\n",
    "input_sequence_length = 32\n",
    "vocab_size = 49408\n",
    "\n",
    "decoder = Decoder(embedding_dim, num_heads, mlp_dimension, num_layers, vocab_size).to(device)\n",
    "#decoder = SimpleCaptionDecoder(embedding_dim, num_heads, mlp_dimension, num_layers, vocab_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    decoder.train()\n",
    "\n",
    "    # Training phase\n",
    "    train_pbar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', mininterval=0.1)\n",
    "\n",
    "    epoch_losses = [] # store all individual losses and calculate the true average at the end\n",
    "\n",
    "    # Iterate over the DataLoader and print the outputs\n",
    "    for batch_idx, (patch_embeddings, text_embeddings, target_ids, mask) in enumerate(train_pbar):\n",
    "        \n",
    "        # Move projection layer to device\n",
    "        image_input_dim = patch_embeddings.shape[2]# get dimension from patch embeddings\n",
    "        image_projection_layer = nn.Linear(image_input_dim, embedding_dim).to(device)\n",
    "        img_features = image_projection_layer(patch_embeddings)\n",
    "        #print('\\nProjected img shape:', img_features.shape)\n",
    "        #print('Projected text shape:', text_embeddings.shape)\n",
    "\n",
    "        # Concatenate image features and text embeddings\n",
    "        decoder_inputs = torch.cat([img_features, text_embeddings], dim=1).to(device)\n",
    "        targets = target_ids.to(device)\n",
    "\n",
    "        # Forward pass through the decoder\n",
    "        logits = decoder(decoder_inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        # Shift logits and targets to align for cross-entropy\n",
    "        #logits = logits[:, :-1].contiguous().view(-1, logits.size(-1))\n",
    "        #targets = target_ids[:, 1:].contiguous().view(-1)\n",
    "        #loss = criterion(logits, targets)\n",
    "\n",
    "        # Calculate loss only on text portion\n",
    "        text_logits = logits[:, -32:, :] # Shape: [batch_size, 32, vocab_size]\n",
    "        text_targets = target_ids[:, -32:]  # Shape: [batch_size, 32]\n",
    "        text_mask = mask[:, -32:] # Shape: [batch_size, 32]\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        text_logits = text_logits.reshape(-1, text_logits.size(-1))\n",
    "        text_targets = text_targets.reshape(-1)\n",
    "        text_mask = text_mask.reshape(-1)\n",
    "\n",
    "        # Calculate loss only on masked positions\n",
    "        loss = criterion(\n",
    "            text_logits[text_mask.bool()],\n",
    "            text_targets[text_mask.bool()]\n",
    "        )\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Store the loss for this batch\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        # Update progress bar with current batch loss\n",
    "        train_pbar.set_postfix({\n",
    "            'batch_loss': f'{loss.item():.4f}'\n",
    "        })\n",
    "        train_pbar.update(1)\n",
    "\n",
    "    # Close progress bar for this epoch\n",
    "    train_pbar.close()\n",
    "    \n",
    "    # Print epoch summary\n",
    "    # Calculate and print training epoch average loss\n",
    "    avg_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(\" \")\n",
    "\n",
    "     # Validation phase\n",
    "    decoder.eval()  # Set model to evaluation mode\n",
    "    val_losses = []  # Store all validation losses\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        val_pbar = tqdm(test_dataloader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]', mininterval=0.1)\n",
    "        \n",
    "        for batch_idx, (patch_embeddings, text_embeddings, target_ids, mask) in enumerate(val_pbar):\n",
    "            # Move projection layer to device\n",
    "            image_input_dim = patch_embeddings.shape[2]\n",
    "            image_projection_layer = nn.Linear(image_input_dim, embedding_dim).to(device)\n",
    "            img_features = image_projection_layer(patch_embeddings)\n",
    "\n",
    "            # Concatenate image features and text embeddings\n",
    "            decoder_inputs = torch.cat([img_features, text_embeddings], dim=1).to(device)\n",
    "            targets = target_ids.to(device)\n",
    "\n",
    "            # Forward pass through the decoder\n",
    "            logits = decoder(decoder_inputs)\n",
    "\n",
    "            # Calculate loss only on text portion\n",
    "            text_logits = logits[:, -32:, :]\n",
    "            text_targets = target_ids[:, -32:]\n",
    "            text_mask = mask[:, -32:]\n",
    "\n",
    "            # Reshape for loss calculation\n",
    "            text_logits = text_logits.reshape(-1, text_logits.size(-1))\n",
    "            text_targets = text_targets.reshape(-1)\n",
    "            text_mask = text_mask.reshape(-1)\n",
    "\n",
    "            # Calculate loss only on masked positions\n",
    "            val_loss = criterion(\n",
    "                text_logits[text_mask.bool()],\n",
    "                text_targets[text_mask.bool()]\n",
    "            )\n",
    "            \n",
    "            # Store validation loss\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Update progress bar with current validation loss\n",
    "            val_pbar.set_postfix({\n",
    "                'val_loss': f'{val_loss.item():.4f}'\n",
    "            })\n",
    "            val_pbar.update(1)\n",
    "    \n",
    "    # Close validation progress bar\n",
    "    val_pbar.close()\n",
    "    \n",
    "    # Calculate average validation loss\n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        \n",
    "print(\"Training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to decoder_08051225.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the model using torch load\n",
    "model_save_path = \"decoder_08051225.pt\"\n",
    "torch.save(decoder, model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DUMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "\n",
    "def train_transformer(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "):\n",
    "    \n",
    "    # Initialize wandb\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_name = f\"full-transformer-{timestamp}\"\n",
    "    wandb.init(project=\"full-transformer-mnist\",\n",
    "               name=run_name,\n",
    "               group=\"experiment-1\",\n",
    "               config={\n",
    "                   \"learning_rate\": learning_rate,\n",
    "                   \"num_epochs\": num_epochs,\n",
    "                   \"batch_size\": train_loader.batch_size,\n",
    "                   \"embedding_dim\": model.embedding_dim,\n",
    "                   \"num_heads\": model.num_heads,\n",
    "                   \"mlp_dimension\": model.mlp_dimension,\n",
    "                   \"num_layers\": model.num_layers,\n",
    "                   \"max_seq_length\": model.num_patches,\n",
    "                   \"vocab_size\": model.vocab_size\n",
    "               })\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # TRAINING LOOP \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Training phase\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', mininterval=1.0) # udpate every 1 second\n",
    "        for batch_idx, (input_images, decoder_inputs, targets) in enumerate(train_loader):\n",
    "            # Move to device\n",
    "            input_images = input_images.to(device)\n",
    "            decoder_inputs = decoder_inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_images, decoder_inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            # outputs.view(-1, outputs.size(-1)) is to flatten the output to a 1D tensor\n",
    "            # targets.view(-1) is to flatten the targets to a 1D tensor\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(-1)\n",
    "            total_correct += (predicted == targets).sum().item()\n",
    "            total_samples += targets.numel()\n",
    "            \n",
    "            # Calculate current accuracy\n",
    "            current_acc = (total_correct / total_samples * 100)\n",
    "            \n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({\n",
    "                'loss': total_loss / (batch_idx + 1),\n",
    "                'acc': f'{current_acc:.2f}%'\n",
    "            })\n",
    "\n",
    "            train_pbar.update(1)\n",
    "        \n",
    "        train_pbar.close()\n",
    "\n",
    "        \n",
    "        # Calculate training metrics after all batches\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = (total_correct / total_samples * 100)\n",
    "        print(f\"\\nTraining completed for epoch {epoch+1}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "        # After training phase\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "            \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "        \n",
    "        # No need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]', mininterval=1.0)\n",
    "            for batch_idx, (images, decoder_input, targets) in enumerate(val_pbar):\n",
    "                # Move to device\n",
    "                images = images.to(device)\n",
    "                decoder_input = decoder_input.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images, decoder_input)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Update validation metrics\n",
    "                _, predicted = outputs.max(-1)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "                val_samples += targets.numel()\n",
    "\n",
    "                # Calculate current validation accuracy\n",
    "                current_val_acc = (val_correct / val_samples * 100)\n",
    "                \n",
    "\n",
    "                # Update progress bar\n",
    "                val_pbar.set_postfix({\n",
    "                    'loss': val_loss / (batch_idx + 1),\n",
    "                    'acc': f'{current_val_acc:.2f}%'\n",
    "                })\n",
    "\n",
    "                val_pbar.update(1)\n",
    "\n",
    "        val_pbar.close()\n",
    "                \n",
    "        # Calculate validation metrics\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = (val_correct / val_samples * 100)\n",
    "\n",
    "        # After validation phase\n",
    "        wandb.log({\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "        \n",
    "\n",
    "      # Print epoch summary\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs} Summary:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "    # After training loop completes\n",
    "    # 1. Save model weights\n",
    "    model_save_path = f\"transformer_weights_{timestamp}_mnistscattered.pt\"\n",
    "\n",
    "    torch.save(model, model_save_path)\n",
    "\n",
    "    print(f\"\\nModel weights saved to {model_save_path}\")\n",
    "\n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Assuming 'data' is one sample from your dataset (e.g., a batch or single entry)\n",
    "image = train['image'][0]  # This will be a PIL.Image object\n",
    "\n",
    "# Check the original image size\n",
    "print(\"Original image size:\", image.size)  # Returns (width, height)\n",
    "\n",
    "# Optionally, check the mode (RGB, grayscale, etc.)\n",
    "print(\"Image mode:\", image.mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions produces a list of captions\n",
    "#captions = [caption for caption_list[0] for caption_list in test['caption']]  \n",
    "\n",
    "captions = []\n",
    "for caption_list in test['caption']:\n",
    "    for caption in caption_list[0]:\n",
    "        captions.append(caption)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change from total_loss and num_batches to epoch_losses = [] offers several advantages:\n",
    "More Accurate Average:\n",
    "With total_loss and num_batches, you're calculating a running average that might be affected by the order of batches\n",
    "With epoch_losses, you store all individual losses and calculate the true average at the end, which is more accurate\n",
    "More Information:\n",
    "By storing all losses in a list, you can:\n",
    "Calculate the median loss\n",
    "Find the minimum and maximum losses\n",
    "Plot the loss distribution\n",
    "Detect outliers or unusual patterns\n",
    "Calculate standard deviation\n",
    "Debugging and Analysis:\n",
    "If you notice something unusual in the average loss, you can look at the individual losses to understand what's happening\n",
    "You can track how the loss changes throughout the epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
